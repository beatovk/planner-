#!/usr/bin/env python3
"""
Web Verifier - –º–æ–¥—É–ª—å –¥–ª—è –≤–µ–±-–ø–æ–∏—Å–∫–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
"""

import requests
import json
import logging
import time
import random
from typing import List, Dict, Optional, Tuple, Any
from urllib.parse import quote_plus
import re

logger = logging.getLogger(__name__)


class WebVerifier:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –≤–µ–±-–ø–æ–∏—Å–∫–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
    """
    
    def __init__(self):
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        ]
        
        self.session = requests.Session()
        self.session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def verify_place_data(self, place_name: str, place_category: str, place_address: str = None) -> Dict[str, Any]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –º–µ—Å—Ç–∞ —á–µ—Ä–µ–∑ –≤–µ–±-–ø–æ–∏—Å–∫
        """
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            search_queries = self._generate_search_queries(place_name, place_category, place_address)
            
            verification_results = []
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å (–º–∞–∫—Å–∏–º—É–º 2-3 –∏—Å—Ç–æ—á–Ω–∏–∫–∞)
            for i, query in enumerate(search_queries[:3]):
                try:
                    result = self._search_place_info(query, place_name)
                    if result:
                        verification_results.append(result)
                    
                    # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    time.sleep(random.uniform(1, 2))
                    
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}': {e}")
                    continue
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            return self._analyze_verification_results(verification_results, place_name, place_category)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–µ—Å—Ç–∞ {place_name}: {e}")
            return {
                "verified": False,
                "confidence": 0.0,
                "sources": [],
                "issues": [f"–û—à–∏–±–∫–∞ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}"],
                "suggestions": []
            }
    
    def _generate_search_queries(self, name: str, category: str, address: str = None) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏"""
        queries = []
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π
        base_query = f'"{name}" {category} Bangkok'
        queries.append(base_query)
        
        # –ó–∞–ø—Ä–æ—Å —Å –∞–¥—Ä–µ—Å–æ–º –µ—Å–ª–∏ –µ—Å—Ç—å
        if address:
            address_query = f'"{name}" "{address}" Bangkok'
            queries.append(address_query)
        
        # –ó–∞–ø—Ä–æ—Å —Ç–æ–ª—å–∫–æ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
        name_query = f'"{name}" restaurant bar Bangkok'
        queries.append(name_query)
        
        return queries
    
    def _search_place_info(self, query: str, place_name: str) -> Optional[Dict[str, Any]]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–µ—Å—Ç–µ"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º DuckDuckGo –¥–ª—è –ø–æ–∏—Å–∫–∞
            search_url = "https://duckduckgo.com/html/"
            params = {
                'q': query,
                'kl': 'en-us'
            }
            
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Referer': 'https://duckduckgo.com/'
            }
            
            response = self.session.get(search_url, params=params, headers=headers, timeout=10)
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            return self._parse_search_results(response.text, place_name)
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è '{query}': {e}")
            return None
    
    def _parse_search_results(self, html: str, place_name: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ HTML (–≤ —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å BeautifulSoup)
            # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –º–µ—Å—Ç–∞
            name_mentions = len(re.findall(re.escape(place_name.lower()), html.lower()))
            
            # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            relevant_keywords = ['restaurant', 'bar', 'cafe', 'bangkok', 'thailand', 'food', 'dining']
            keyword_matches = sum(1 for keyword in relevant_keywords if keyword.lower() in html.lower())
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–∞–π—Ç—ã
            relevant_domains = ['tripadvisor', 'google', 'foursquare', 'zomato', 'timeout', 'bk.asia-city']
            domain_matches = sum(1 for domain in relevant_domains if domain in html.lower())
            
            return {
                "name_mentions": name_mentions,
                "keyword_matches": keyword_matches,
                "domain_matches": domain_matches,
                "relevance_score": (name_mentions * 2 + keyword_matches + domain_matches) / 10
            }
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
            return None
    
    def _analyze_verification_results(self, results: List[Dict], place_name: str, place_category: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        if not results:
            return {
                "verified": False,
                "confidence": 0.0,
                "sources": [],
                "issues": ["–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"],
                "suggestions": ["–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞–ø–∏—Å–∞–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è"]
            }
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–π score
        total_score = sum(r.get("relevance_score", 0) for r in results)
        avg_score = total_score / len(results)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –¥–æ–≤–µ—Ä–∏—è
        if avg_score >= 0.7:
            confidence = "high"
            verified = True
        elif avg_score >= 0.4:
            confidence = "medium"
            verified = True
        else:
            confidence = "low"
            verified = False
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º
        issues = []
        suggestions = []
        
        if avg_score < 0.3:
            issues.append("–ù–∏–∑–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –≤ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö")
            suggestions.append("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏")
        
        if not any(r.get("domain_matches", 0) > 0 for r in results):
            issues.append("–ù–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –Ω–∞ –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö —Å–∞–π—Ç–∞—Ö")
            suggestions.append("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –º–µ—Å—Ç–∞")
        
        return {
            "verified": verified,
            "confidence": avg_score,
            "sources": len(results),
            "issues": issues,
            "suggestions": suggestions,
            "details": results
        }
    
    def search_quality_images(self, place_name: str, place_category: str) -> List[str]:
        """
        –ò—â–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–µ—Å—Ç–∞
        """
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            image_query = f"{place_name} {place_category} Bangkok professional photo"
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º DuckDuckGo –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            search_url = "https://duckduckgo.com/"
            params = {
                'q': image_query,
                'iax': 'images',
                'ia': 'images'
            }
            
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Referer': 'https://duckduckgo.com/'
            }
            
            response = self.session.get(search_url, params=params, headers=headers, timeout=10)
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            return self._parse_image_results(response.text)
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è '{place_name}': {e}")
            return []
    
    def _parse_image_results(self, html: str) -> List[str]:
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
            image_urls = []
            
            # –ò—â–µ–º URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ HTML
            img_pattern = r'https://[^"\s]+\.(?:jpg|jpeg|png|webp)(?:\?[^"\s]*)?'
            matches = re.findall(img_pattern, html, re.IGNORECASE)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            for url in matches[:10]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10
                if self._is_quality_image_url(url):
                    image_urls.append(url)
            
            return image_urls[:5]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∞–∫—Å–∏–º—É–º 5 –ª—É—á—à–∏—Ö
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}")
            return []
    
    def _is_quality_image_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ URL"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
            quality_extensions = ['.jpg', '.jpeg', '.png', '.webp']
            if not any(url.lower().endswith(ext) for ext in quality_extensions):
                return False
            
            # –ò—Å–∫–ª—é—á–∞–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º URL)
            if any(param in url.lower() for param in ['thumb', 'small', 'icon', 'avatar']):
                return False
            
            # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞
            if any(param in url.lower() for param in ['w=', 'width=', 'h=', 'height=']):
                return True
            
            # –ò—Å–∫–ª—é—á–∞–µ–º —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ –∏ –∞–≤–∞—Ç–∞—Ä—ã
            if any(domain in url.lower() for domain in ['facebook', 'instagram', 'twitter', 'avatar']):
                return False
            
            return True
            
        except Exception:
            return False


def main():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ WebVerifier"""
    verifier = WebVerifier()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_place = {
        "name": "Sirocco Sky Bar",
        "category": "Bar",
        "address": "Lebua at State Tower, Bangkok"
    }
    
    print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ WebVerifier...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å
    verification = verifier.verify_place_data(
        test_place["name"],
        test_place["category"],
        test_place["address"]
    )
    
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏: {verification}")
    
    # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    images = verifier.search_quality_images(
        test_place["name"],
        test_place["category"]
    )
    
    print(f"–ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(images)}")
    for i, img in enumerate(images[:3], 1):
        print(f"  {i}. {img}")


if __name__ == "__main__":
    main()
