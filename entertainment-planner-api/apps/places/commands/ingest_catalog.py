"""
–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –º–∞—Å—Å–æ–≤–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ BK Magazine
"""
import logging
from typing import List, Dict, Any
from apps.core.db import SessionLocal
from apps.places.models import Place
from apps.places.ingestion.bk_magazine_adapter import BKMagazineAdapter
from apps.places.workers.gpt_normalizer import GPTNormalizerWorker
from apps.places.commands.enrich_bk_google import enrich_bk_places
from datetime import datetime

logger = logging.getLogger(__name__)


def ingest_catalog(catalog_url: str, limit: int = None, max_pages: int = None, dry_run: bool = False):
    """
    –ú–∞—Å—Å–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–∞–ª–æ–≥–∞ BK Magazine
    
    Args:
        catalog_url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞
        limit: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç–∞—Ç–µ–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        dry_run: –†–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î
    """
    print(f"üöÄ –ú–ê–°–°–û–í–´–ô –ü–ê–†–°–ò–ù–ì –ö–ê–¢–ê–õ–û–ì–ê BK MAGAZINE")
    print(f"URL: {catalog_url}")
    print(f"–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π: {limit or '–ù–µ—Ç'}")
    print(f"–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü: {max_pages or '–ù–µ—Ç'}")
    print(f"–†–µ–∂–∏–º: {'–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ' if dry_run else '–ü—Ä–æ–¥–∞–∫—à–Ω'}")
    print("=" * 70)
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–¥–∞–ø—Ç–µ—Ä–∞
        adapter = BKMagazineAdapter()
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–∞–ª–æ–≥–∞ –∏ —Å—Ç–∞—Ç–µ–π
        print("\\nüìñ –ü–ê–†–°–ò–ù–ì –ö–ê–¢–ê–õ–û–ì–ê –ò –°–¢–ê–¢–ï–ô...")
        places = adapter.parse_catalog_articles(catalog_url, limit=limit, max_pages=max_pages)
        
        if not places:
            print("‚ùå –ú–µ—Å—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
            return
        
        print(f"\\nüìä –ù–ê–ô–î–ï–ù–û –ú–ï–°–¢: {len(places)}")
        
        if dry_run:
            print("\\nüîç –†–ï–ñ–ò–ú –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø - –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã:")
            for i, place in enumerate(places[:5], 1):
                print(f"{i:2d}. {place['title']}")
                print(f"    –°—Ç–∞—Ç—å—è: {place.get('article_title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
                print(f"    –û–ø–∏—Å–∞–Ω–∏–µ: {place['teaser'][:100]}...")
                print()
            return
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        print("\\nüíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –í –ë–î...")
        db = SessionLocal()
        added_count = 0
        skipped_count = 0
        total_places = len(places)
        
        try:
            for i, place_data in enumerate(places, 1):
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 –º–µ—Å—Ç
                if i % 10 == 0 or i == total_places:
                    print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i}/{total_places} –º–µ—Å—Ç...")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–æ–µ –º–µ—Å—Ç–æ
                existing = db.query(Place).filter(
                    Place.name == place_data['title'],
                    Place.source == 'bk_magazine'
                ).first()
                
                if existing:
                    skipped_count += 1
                    continue
                
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ –º–µ—Å—Ç–æ
                place = Place(
                    source='bk_magazine',
                    source_url=place_data.get('article_url', catalog_url),
                    raw_payload=f"<article>{place_data['title']}</article>",
                    scraped_at=datetime.utcnow(),
                    name=place_data['title'],
                    category='Restaurant',
                    description_full=place_data['teaser'],
                    processing_status='new'
                )
                
                db.add(place)
                added_count += 1
                
                if added_count % 10 == 0:
                    print(f"   –î–æ–±–∞–≤–ª–µ–Ω–æ: {added_count} –º–µ—Å—Ç...")
            
            db.commit()
            print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ë–î: {added_count} –Ω–æ–≤—ã—Ö –º–µ—Å—Ç")
            print(f"‚û°Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_count} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Å—Ç")
            
        except Exception as e:
            db.rollback()
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î: {e}")
            return
        finally:
            db.close()
        
        # –°–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ GPT
        print("\\nü§ñ –°–ê–ú–ú–ê–†–ò–ó–ê–¶–ò–Ø –ß–ï–†–ï–ó GPT...")
        try:
            worker = GPTNormalizerWorker()
            worker.run()
            print("‚úÖ –°–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        
        # –û–±–æ–≥–∞—â–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Google API
        print("\\nüåç –û–ë–û–ì–ê–©–ï–ù–ò–ï –ß–ï–†–ï–ó GOOGLE API...")
        try:
            enrich_bk_places(batch_size=50, dry_run=False)
            print("‚úÖ –û–±–æ–≥–∞—â–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è: {e}")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\\nüìà –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        db = SessionLocal()
        try:
            total_bk = db.query(Place).filter(Place.source == 'bk_magazine').count()
            with_coords = db.query(Place).filter(
                Place.source == 'bk_magazine',
                Place.lat.isnot(None),
                Place.lng.isnot(None)
            ).count()
            with_gmaps_id = db.query(Place).filter(
                Place.source == 'bk_magazine',
                Place.gmaps_place_id.isnot(None)
            ).count()
            
            print(f"–í—Å–µ–≥–æ –º–µ—Å—Ç BK Magazine: {total_bk}")
            print(f"–° –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏: {with_coords} ({with_coords/total_bk*100:.1f}%)")
            print(f"–° Google ID: {with_gmaps_id} ({with_gmaps_id/total_bk*100:.1f}%)")
            
        finally:
            db.close()
        
        print("\\nüéâ –ú–ê–°–°–û–í–´–ô –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù!")
        
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ –º–∞—Å—Å–æ–≤–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–∞–ª–æ–≥–∞: {e}")


if __name__ == "__main__":
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    catalog_url = "https://bk.asia-city.com/search-news?type=restaurant"
    
    # –ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (360 —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤)
    print("üöÄ –ü–û–õ–ù–´–ô –ü–ê–†–°–ò–ù–ì –í–°–ï–• –°–¢–†–ê–ù–ò–¶ (360 —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤):")
    ingest_catalog(catalog_url, max_pages=None, dry_run=False)
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è —Ç–µ—Å—Ç–æ–≤)
    # print("\\nüß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –° –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï–ú 2 –°–¢–†–ê–ù–ò–¶–´ (48 —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤):")
    # ingest_catalog(catalog_url, max_pages=2, dry_run=True)
    
    # –ü–∞—Ä—Å–∏–Ω–≥ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
    # print("\\nüöÄ –ü–ê–†–°–ò–ù–ì 5 –°–¢–†–ê–ù–ò–¶ (120 —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤):")
    # ingest_catalog(catalog_url, max_pages=5, dry_run=False)
