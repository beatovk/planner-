#!/usr/bin/env python3
"""
–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–ø-–∫–ª—É–±–æ–≤ –∏–∑ CSV —Ñ–∞–π–ª–∞
"""

import csv
import os
import sys
from datetime import datetime
from sqlalchemy.exc import IntegrityError

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from apps.core.db import SessionLocal
from apps.places.models import Place

def add_top_clubs(file_path: str):
    """–î–æ–±–∞–≤–∏—Ç—å —Ç–æ–ø-–∫–ª—É–±—ã –∏–∑ CSV"""
    db = SessionLocal()
    added_count = 0
    skipped_count = 0
    total_rows = 0
    used_urls = set()

    try:
        with open(file_path, mode='r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            places_to_add = []
            
            for row in reader:
                total_rows += 1
                name = row.get('name')
                description_full = row.get('description_full')
                source_url = row.get('source_url')

                if not name or not description_full:
                    print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ –º–µ—Å—Ç–æ –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –∏–º–µ–Ω–∏ –∏–ª–∏ –æ–ø–∏—Å–∞–Ω–∏—è: {row}")
                    skipped_count += 1
                    continue

                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π source_url
                if not source_url:
                    base_url = f"top_clubs_batch_01_{name.lower().replace(' ', '_').replace('(', '').replace(')', '').replace('‚Äî', '_').replace('.', '').replace(',', '').replace('&', 'and')}"
                else:
                    base_url = f"top_clubs_{source_url}"
                
                # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ URL —É–Ω–∏–∫–∞–ª–µ–Ω
                source_url = base_url
                counter = 1
                while source_url in used_urls:
                    source_url = f"{base_url}_{counter}"
                    counter += 1
                
                used_urls.add(source_url)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                existing_place = db.query(Place).filter(
                    (Place.name == name) | (Place.source_url == source_url)
                ).first()

                if existing_place:
                    skipped_count += 1
                    continue

                # –í—Å–µ –º–µ—Å—Ç–∞ –∏–∑ —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞ - –∫–ª—É–±—ã
                new_place = Place(
                    name=name,
                    description_full=description_full,
                    category="club",
                    processing_status="new",
                    source="top_clubs_batch_01",
                    source_url=source_url,
                    scraped_at=datetime.now()
                )
                places_to_add.append(new_place)

            if places_to_add:
                db.bulk_save_objects(places_to_add)
                db.commit()
                added_count = len(places_to_add)
                print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {added_count} –Ω–æ–≤—ã—Ö –∫–ª—É–±–æ–≤")

    except IntegrityError as e:
        db.rollback()
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
    except Exception as e:
        db.rollback()
        print(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
    finally:
        db.close()

    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç:")
    print(f"  –î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö –º–µ—Å—Ç: {added_count}")
    print(f"  –ü—Ä–æ–ø—É—â–µ–Ω–æ (—É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç): {skipped_count}")
    print(f"  –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_rows}")

if __name__ == "__main__":
    csv_file_path = '../docs/places.csv/+Top_Clubs__Batch_01_.csv'
    print(f"üöÄ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–ø-–∫–ª—É–±–æ–≤ –∏–∑ {csv_file_path.split('/')[-1]}\n" + "="*60)
    add_top_clubs(csv_file_path)
