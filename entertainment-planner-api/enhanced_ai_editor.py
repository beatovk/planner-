#!/usr/bin/env python3
"""
Enhanced AI Editor Agent - —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥–æ–º URL –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
–ü–æ–ª—É—á–∞–µ—Ç –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ —Å–∂–∏–º–∞–µ—Ç –¥–æ –≥–ª–∞–≤–Ω–æ–π —Å—É—Ç–∏
"""

import os
import sys
import json
import logging
import requests
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime
import time
import random
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, quote_plus

# Add project root to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from apps.core.db import SessionLocal
from apps.places.models import Place
from openai import OpenAI
from sqlalchemy.orm import Session

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class EnhancedAIEditorAgent:
    """
    Enhanced AI Editor Agent - –ø–æ–ª—É—á–∞–µ—Ç –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑ URL –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    """
    
    def __init__(self, api_key: str = None, batch_size: int = 10):
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        self.batch_size = batch_size
        self.client = OpenAI(api_key=self.api_key)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.processed_count = 0
        self.scraped_count = 0
        self.web_search_count = 0
        self.compressed_count = 0
        self.updated_count = 0
        self.error_count = 0
        
        # User-Agent –¥–ª—è –≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥–∞
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
    
    def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞"""
        logger.info("üöÄ Starting Enhanced AI Editor Agent...")
        
        try:
            self._process_incomplete_places()
            
            logger.info("‚úÖ Enhanced AI Editor Agent completed!")
            self._print_stats()
            
        except Exception as e:
            logger.error(f"‚ùå Critical error: {e}")
            raise
    
    def _process_incomplete_places(self):
        """Special processing for places without description and summary"""
        # Get places without description and summary
        db = SessionLocal()
        try:
            places = db.query(Place).filter(
                (Place.description_full.is_(None) | (Place.description_full == '')) &
                (Place.summary.is_(None) | (Place.summary == ''))
            ).limit(self.batch_size).all()
            
            if not places:
                logger.info("No places without description and summary to process")
                return
            
            logger.info(f"Found {len(places)} places without description and summary")
        finally:
            db.close()
        
        # Process each place individually with its own connection
        for place in places:
            try:
                self._process_incomplete_place_individual(place)
            except Exception as e:
                logger.error(f"Error processing place {place.id}: {e}")
                self.error_count += 1
        
        self.processed_count += len(places)
        logger.info(f"‚úÖ Successfully processed {len(places)} places")
    
    def _process_incomplete_place_individual(self, place: Place):
        """Process one place without description and summary with individual connection"""
        logger.info(f"üîç Processing place: {place.name}")
        
        description = None
        
        # 1. Check if there's a valid source URL
        if place.source_url and place.source_url.strip() and not place.source_url.startswith('timeout_'):
            logger.info(f"üåê Found source URL: {place.source_url}")
            
            # 2. Try to get description from URL
            description = self._scrape_description_from_url(place.source_url, place)
            
            if description:
                logger.info(f"‚úÖ Got description from URL ({len(description)} characters)")
                self.scraped_count += 1
            else:
                logger.warning(f"‚ùå Failed to get description from URL for {place.name}")
        
        # 3. If no description from URL, try web search
        if not description:
            logger.info(f"üîç No valid URL, trying web search for: {place.name}")
            description = self._web_search_description(place)
            
            if description:
                logger.info(f"‚úÖ Got description from web search ({len(description)} characters)")
                self.web_search_count += 1
            else:
                logger.warning(f"‚ùå Failed to get description from web search for {place.name}")
        
        # 4. If we have description, compress and update
        if description:
            # Compress description to main essence
            compressed_description = self._compress_description(description, place)
            
            if compressed_description:
                logger.info(f"üìù Compressed to {len(compressed_description)} characters")
                
                # Update place with individual connection
                self._update_place_individual(place, compressed_description)
                
                self.compressed_count += 1
                self.updated_count += 1
                
                logger.info(f"‚úÖ Place {place.name} updated and sent for summarization")
            else:
                logger.warning(f"‚ùå Failed to compress description for {place.name}")
        else:
            logger.warning(f"‚ùå No description found for {place.name} from any source")
    
    def _update_place_individual(self, place: Place, description: str):
        """Update place with individual database connection"""
        db = SessionLocal()
        try:
            # Get fresh place from database
            db_place = db.query(Place).filter(Place.id == place.id).first()
            if db_place:
                db_place.description_full = description
                db_place.processing_status = 'new'  # Send for summarization
                db_place.updated_at = datetime.now()
                db.commit()
                logger.info(f"‚úÖ Place {place.name} updated in database")
            else:
                logger.error(f"‚ùå Place {place.id} not found in database")
        except Exception as e:
            logger.error(f"‚ùå Database update error for place {place.id}: {e}")
            db.rollback()
            raise
        finally:
            db.close()
    
    def _scrape_description_from_url(self, url: str, place: Place) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è —Å URL –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        try:
            logger.info(f"üåê –°–∫—Ä–∞–ø–∏–Ω–≥ URL: {url}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(random.uniform(1, 3))
            
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
            if 'timeout.com' in url:
                return self._extract_timeout_description(soup, place)
            elif 'bk.asia-city.com' in url:
                return self._extract_bk_magazine_description(soup, place)
            else:
                return self._extract_generic_description(soup, place)
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ URL {url}: {e}")
            return None
    
    def _extract_timeout_description(self, soup: BeautifulSoup, place: Place) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è —Å TimeOut"""
        try:
            # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã TimeOut
            selectors = [
                'div[data-testid="article-body"] p',
                '.article-body p',
                '.content p',
                'article p',
                '.description p',
                'div[class*="content"] p',
                'div[class*="article"] p'
            ]
            
            for selector in selectors:
                paragraphs = soup.select(selector)
                if paragraphs:
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
                    text = ' '.join([p.get_text(strip=True) for p in paragraphs])
                    if len(text) > 100:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
                        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –æ–ø–∏—Å–∞–Ω–∏–µ TimeOut —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä: {selector}")
                        return text
            
            # Fallback - –∏—â–µ–º –ª—é–±–æ–π —Ç–µ–∫—Å—Ç –≤ —Å—Ç–∞—Ç—å–µ
            article_text = soup.get_text()
            if len(article_text) > 200:
                logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –¥–ª—è TimeOut")
                return article_text
                
            return None
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è TimeOut –æ–ø–∏—Å–∞–Ω–∏—è: {e}")
            return None
    
    def _extract_bk_magazine_description(self, soup: BeautifulSoup, place: Place) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è —Å BK Magazine"""
        try:
            # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã BK Magazine
            selectors = [
                '.entry-content p',
                '.post-content p',
                '.article-content p',
                '.content p',
                'article p',
                '.description p'
            ]
            
            for selector in selectors:
                paragraphs = soup.select(selector)
                if paragraphs:
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
                    text = ' '.join([p.get_text(strip=True) for p in paragraphs])
                    if len(text) > 100:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
                        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –æ–ø–∏—Å–∞–Ω–∏–µ BK Magazine —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä: {selector}")
                        return text
            
            # Fallback - –∏—â–µ–º –ª—é–±–æ–π —Ç–µ–∫—Å—Ç –≤ —Å—Ç–∞—Ç—å–µ
            article_text = soup.get_text()
            if len(article_text) > 200:
                logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –¥–ª—è BK Magazine")
                return article_text
                
            return None
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è BK Magazine –æ–ø–∏—Å–∞–Ω–∏—è: {e}")
            return None
    
    def _extract_generic_description(self, soup: BeautifulSoup, place: Place) -> Optional[str]:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è"""
        try:
            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()
            
            # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            selectors = [
                'article p',
                '.content p',
                '.description p',
                '.article p',
                'main p',
                'div[class*="content"] p',
                'div[class*="article"] p'
            ]
            
            for selector in selectors:
                paragraphs = soup.select(selector)
                if paragraphs:
                    text = ' '.join([p.get_text(strip=True) for p in paragraphs])
                    if len(text) > 100:
                        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –æ–ø–∏—Å–∞–Ω–∏–µ —á–µ—Ä–µ–∑ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä: {selector}")
                        return text
            
            return None
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {e}")
            return None
    
    def _web_search_description(self, place: Place) -> Optional[str]:
        """–ü–æ–∏—Å–∫ –æ–ø–∏—Å–∞–Ω–∏—è –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –º–µ—Å—Ç–∞"""
        try:
            # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ç –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤ –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
            clean_name = self._clean_place_name(place.name)
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–∞–π—Ç—ã —Å —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞–º–∏ –ë–∞–Ω–≥–∫–æ–∫–∞
            known_sites = self._get_known_restaurant_sites(clean_name)
            
            for url, site_name in known_sites:
                try:
                    logger.info(f"üåê Trying known site: {site_name} - {url}")
                    description = self._scrape_description_from_url(url, place)
                    
                    if description and len(description) > 100:
                        logger.info(f"‚úÖ Found description from known site: {url}")
                        return description
                        
                except Exception as e:
                    logger.warning(f"Failed to scrape {url}: {e}")
                    continue
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–∞–π—Ç–∞—Ö, –ø—Ä–æ–±—É–µ–º DuckDuckGo
            logger.info(f"üîç Trying DuckDuckGo search for: {clean_name}")
            search_query = f"{clean_name} Bangkok restaurant bar cafe"
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(random.uniform(2, 4))
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo
            search_url = f"https://duckduckgo.com/html/?q={quote_plus(search_query)}"
            
            response = requests.get(search_url, headers=self.headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ DuckDuckGo
            search_results = self._extract_duckduckgo_results(soup, place)
            
            if not search_results:
                logger.warning(f"No search results found for {place.name}")
                return None
            
            # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ —Å –∫–∞–∂–¥–æ–π —Å—Å—ã–ª–∫–∏
            for url, title in search_results[:3]:  # –¢–æ–ø-3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                try:
                    logger.info(f"üåê Trying search result: {url}")
                    description = self._scrape_description_from_url(url, place)
                    
                    if description and len(description) > 100:
                        logger.info(f"‚úÖ Found description from web search: {url}")
                        return description
                        
                except Exception as e:
                    logger.warning(f"Failed to scrape {url}: {e}")
                    continue
            
            logger.warning(f"No valid descriptions found from web search for {place.name}")
            return None
            
        except Exception as e:
            logger.error(f"Error in web search for {place.name}: {e}")
            return None
    
    def _get_known_restaurant_sites(self, clean_name: str) -> List[Tuple[str, str]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ —Å —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞–º–∏ –ë–∞–Ω–≥–∫–æ–∫–∞"""
        sites = []
        
        # TimeOut Bangkok
        timeout_url = f"https://www.timeout.com/bangkok/search?q={quote_plus(clean_name)}"
        sites.append((timeout_url, "TimeOut Bangkok"))
        
        # BK Magazine
        bk_url = f"https://bk.asia-city.com/search?q={quote_plus(clean_name)}"
        sites.append((bk_url, "BK Magazine"))
        
        # Google Maps (–µ—Å–ª–∏ –µ—Å—Ç—å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã)
        # sites.append((f"https://www.google.com/maps/search/{quote_plus(clean_name)}+Bangkok", "Google Maps"))
        
        return sites
    
    def _clean_place_name(self, name: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è –º–µ—Å—Ç–∞ –æ—Ç –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤ –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å—ã —Ç–∏–ø–∞ "1. ", "2. ", "10. "
        name = re.sub(r'^\d+\.\s*', '', name)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        name = name.strip()
        
        return name
    
    def _extract_duckduckgo_results(self, soup: BeautifulSoup, place: Place) -> List[Tuple[str, str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –∏–∑ DuckDuckGo"""
        try:
            results = []
            all_links = []
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞ DuckDuckGo
            for link in soup.find_all('a', class_='result__a'):
                href = link.get('href')
                title = link.get_text(strip=True)
                all_links.append((href, title))
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                if (href and 
                    href.startswith('http') and 
                    not href.startswith('https://duckduckgo.com') and
                    not href.startswith('https://maps.') and
                    not href.startswith('https://translate.') and
                    title and len(title) > 10):
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
                    if self._is_relevant_result(title, place.name):
                        results.append((href, title))
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ –∫–ª–∞—Å—Å, –ø—Ä–æ–±—É–µ–º –æ–±—â–∏–π –ø–æ–∏—Å–∫
            if not results:
                logger.info(f"No results from result__a class, trying general search...")
                for link in soup.find_all('a', href=True):
                    href = link.get('href')
                    title = link.get_text(strip=True)
                    all_links.append((href, title))
                    
                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                    if (href and 
                        href.startswith('http') and 
                        not href.startswith('https://duckduckgo.com') and
                        not href.startswith('https://maps.') and
                        not href.startswith('https://translate.') and
                        title and len(title) > 10):
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
                        if self._is_relevant_result(title, place.name):
                            results.append((href, title))
            
            # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            logger.info(f"Total links found: {len(all_links)}")
            logger.info(f"Valid HTTP links: {len([l for l in all_links if l[0] and l[0].startswith('http')])}")
            
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            seen = set()
            unique_results = []
            for url, title in results:
                if url not in seen:
                    seen.add(url)
                    unique_results.append((url, title))
            
            logger.info(f"Found {len(unique_results)} relevant DuckDuckGo search results")
            return unique_results
            
        except Exception as e:
            logger.error(f"Error extracting DuckDuckGo search results: {e}")
            return []
    
    def _is_relevant_result(self, title: str, place_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–æ–∏—Å–∫–∞"""
        try:
            # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            clean_title = re.sub(r'[^\w\s]', '', title.lower())
            clean_place = re.sub(r'[^\w\s]', '', place_name.lower())
            
            # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å—ã
            clean_place = re.sub(r'^\d+\.\s*', '', clean_place)
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
            title_words = set(clean_title.split())
            place_words = set(clean_place.split())
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            common_words = title_words.intersection(place_words)
            
            # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            is_relevant = (
                len(common_words) > 0 or  # –ï—Å—Ç—å –æ–±—â–∏–µ —Å–ª–æ–≤–∞
                any(word in clean_title for word in clean_place.split() if len(word) > 3) or  # –ï—Å—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
                clean_place in clean_title  # –ù–∞–∑–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
            ) and len(clean_title) > 15  # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
            
            if is_relevant:
                logger.info(f"‚úÖ Relevant result: '{title}' (common words: {common_words})")
            else:
                logger.debug(f"‚ùå Not relevant: '{title}' (common words: {common_words})")
            
            return is_relevant
            
        except Exception as e:
            logger.error(f"Error checking relevance: {e}")
            return False
    
    def _compress_description(self, description: str, place: Place) -> Optional[str]:
        """–°–∂–∞—Ç–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –¥–æ –≥–ª–∞–≤–Ω–æ–π —Å—É—Ç–∏ (6-10 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)"""
        try:
            if not description or len(description.strip()) < 50:
                return None
            
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
            cleaned_text = self._clean_text(description)
            
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —É–∂–µ –∫–æ—Ä–æ—Ç–∫–∏–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            if len(cleaned_text) <= 500:
                return cleaned_text
            
            # Use GPT to compress to main essence
            prompt = f"""Compress this restaurant/place description to its main essence (6-10 sentences):

Name: {place.name}
Category: {place.category}

Original description:
{cleaned_text[:2000]}

Requirements:
- Keep only the most important information
- Maximum 6-10 sentences
- Focus on unique features, atmosphere, cuisine
- Remove repetitions and unnecessary details
- Preserve factual information

Compressed description:"""
            
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=300
            )
            
            compressed = response.choices[0].message.content.strip()
            
            if compressed and len(compressed) > 50:
                logger.info(f"Description compressed from {len(cleaned_text)} to {len(compressed)} characters")
                return compressed
            else:
                logger.warning("GPT returned too short compressed description")
                return cleaned_text[:500] + "..." if len(cleaned_text) > 500 else cleaned_text
                
        except Exception as e:
            logger.error(f"Error compressing description: {e}")
            # Fallback - just truncate to 500 characters
            return description[:500] + "..." if len(description) > 500 else description
    
    def _clean_text(self, text: str) -> str:
        """Clean text from unnecessary characters"""
        # Remove extra spaces and line breaks
        text = re.sub(r'\s+', ' ', text)
        # Remove HTML tags if any remain
        text = re.sub(r'<[^>]+>', '', text)
        # Remove unnecessary characters
        text = re.sub(r'[^\w\s.,!?;:-]', '', text)
        return text.strip()
    
    def _mark_as_error(self, place: Place, error: str, db: Session):
        """Mark place as erroneous"""
        place.processing_status = 'error'
        place.last_error = error
        place.updated_at = datetime.now()
        db.add(place)
    
    def _print_stats(self):
        """Print work statistics"""
        logger.info("üìä Enhanced AI Editor Agent Statistics:")
        logger.info(f"  Processed places: {self.processed_count}")
        logger.info(f"  Scraped descriptions: {self.scraped_count}")
        logger.info(f"  Web search descriptions: {self.web_search_count}")
        logger.info(f"  Compressed descriptions: {self.compressed_count}")
        logger.info(f"  Updated places: {self.updated_count}")
        logger.info(f"  Errors: {self.error_count}")


def main():
    """Main function"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Enhanced AI Editor Agent')
    parser.add_argument('--batch-size', type=int, default=10, help='Batch size')
    parser.add_argument('--api-key', type=str, help='OpenAI API key')
    parser.add_argument('--verbose', '-v', action='store_true', help='Verbose logging')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Set API key
    if args.api_key:
        os.environ['OPENAI_API_KEY'] = args.api_key
    
    try:
        agent = EnhancedAIEditorAgent(
            api_key=args.api_key,
            batch_size=args.batch_size
        )
        
        print("ü§ñ Starting Enhanced AI Editor Agent...")
        print(f"üìä Batch size: {args.batch_size}")
        print(f"üîë API key: {'set' if os.getenv('OPENAI_API_KEY') else 'NOT FOUND'}")
        print("-" * 50)
        
        agent.run()
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
