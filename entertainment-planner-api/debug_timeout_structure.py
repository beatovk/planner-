#!/usr/bin/env python3
"""
–û—Ç–ª–∞–¥–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã TimeOut —Å—Ç—Ä–∞–Ω–∏—Ü
"""

import requests
from bs4 import BeautifulSoup
import time

def debug_page_structure(url):
    print(f"\nüîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º: {url}")
    print("=" * 80)
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # –ò—â–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –º–µ—Å—Ç
        selectors_to_try = [
            'h3 a[href*="/bangkok/restaurants/"]',
            'h2 a[href*="/bangkok/restaurants/"]',
            'h1 a[href*="/bangkok/restaurants/"]',
            'a[href*="/bangkok/restaurants/"]',
            '.listing-item a[href*="/bangkok/restaurants/"]',
            '.place-item a[href*="/bangkok/restaurants/"]',
            '.venue-item a[href*="/bangkok/restaurants/"]',
            'article a[href*="/bangkok/restaurants/"]',
            '.card a[href*="/bangkok/restaurants/"]',
            '.item a[href*="/bangkok/restaurants/"]'
        ]
        
        print("üîç –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã:")
        for selector in selectors_to_try:
            links = soup.select(selector)
            print(f"  {selector}: {len(links)} —Å—Å—ã–ª–æ–∫")
            if links:
                for i, link in enumerate(links[:3]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                    print(f"    {i+1}. {link.get_text().strip()[:50]} -> {link.get('href')}")
        
        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å—Ç–∞—Ç–µ–π
        print("\nüì∞ –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å—Ç–∞—Ç–µ–π:")
        article_titles = soup.find_all(['h1', 'h2', 'h3'], string=lambda text: text and 'restaurant' in text.lower())
        for i, title in enumerate(article_titles[:5]):
            print(f"  {i+1}. {title.get_text().strip()}")
        
        # –ò—â–µ–º —Å–ø–∏—Å–∫–∏ –º–µ—Å—Ç
        print("\nüìã –°–ø–∏—Å–∫–∏ –º–µ—Å—Ç:")
        list_items = soup.find_all(['li', 'div'], class_=lambda x: x and any(word in x.lower() for word in ['place', 'venue', 'restaurant', 'cafe', 'item', 'listing']))
        print(f"  –ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å–ø–∏—Å–∫–∞: {len(list_items)}")
        
        # –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —Å –º–µ—Å—Ç–∞–º–∏
        print("\nüè¢ –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —Å –º–µ—Å—Ç–∞–º–∏:")
        containers = soup.find_all(['div', 'section', 'article'], class_=lambda x: x and any(word in x.lower() for word in ['list', 'places', 'venues', 'restaurants', 'cafes', 'items']))
        for i, container in enumerate(containers[:3]):
            print(f"  {i+1}. {container.get('class')} - {len(container.find_all('a'))} —Å—Å—ã–ª–æ–∫")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        with open(f'debug_{url.split("/")[-1]}.html', 'w', encoding='utf-8') as f:
            f.write(soup.prettify())
        print(f"\nüíæ HTML —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ debug_{url.split('/')[-1]}.html")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    test_urls = [
        'https://www.timeout.com/bangkok/restaurants/best-restaurants-and-cafes-asoke',
        'https://www.timeout.com/bangkok/restaurants/best-places-to-eat-iconsiam',
        'https://www.timeout.com/bangkok/news/thailand-leads-asias-50-best-restaurants-2025-032625'
    ]
    
    for url in test_urls:
        debug_page_structure(url)
        time.sleep(2)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
