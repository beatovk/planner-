#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–¥–∞–ø—Ç–µ—Ä –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ TimeOut Bangkok
"""

import requests
from bs4 import BeautifulSoup
import re
import json
import time
from typing import List, Dict, Any

class EnhancedTimeOutAdapter:
    def __init__(self):
        self.base_url = "https://www.timeout.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def parse_list_page(self, url: str) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ —Å–ø–∏—Å–∫–æ–º –º–µ—Å—Ç"""
        print(f"üîç –ü–∞—Ä—Å–∏–Ω–≥: {url}")
        
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            places = []
            
            # –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º —Å –Ω–æ–º–µ—Ä–∞–º–∏
            places.extend(self._extract_numbered_places(soup))
            
            # –ú–µ—Ç–æ–¥ 2: –ü–æ–∏—Å–∫ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞—Ö —Å–ø–∏—Å–∫–æ–≤
            places.extend(self._extract_from_containers(soup))
            
            # –ú–µ—Ç–æ–¥ 3: –ü–æ–∏—Å–∫ –ø–æ —Å—Å—ã–ª–∫–∞–º –Ω–∞ —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã
            places.extend(self._extract_restaurant_links(soup))
            
            # –ú–µ—Ç–æ–¥ 4: –ü–æ–∏—Å–∫ –ø–æ JSON-LD —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º
            places.extend(self._extract_from_json_ld(soup))
            
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            places = self._remove_duplicates(places)
            
            print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –º–µ—Å—Ç: {len(places)}")
            return places
            
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞: {e}")
            return []
    
    def _extract_numbered_places(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –º–µ—Å—Ç —Å –Ω–æ–º–µ—Ä–∞–º–∏ (1., 2., 3. –∏ —Ç.–¥.)"""
        places = []
        
        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å –Ω–æ–º–µ—Ä–∞–º–∏
        numbered_patterns = [
            r'^\d+\.\s+(.+)',  # 1. –ù–∞–∑–≤–∞–Ω–∏–µ
            r'^\d+\s+(.+)',    # 1 –ù–∞–∑–≤–∞–Ω–∏–µ
            r'^\(\d+\)\s+(.+)' # (1) –ù–∞–∑–≤–∞–Ω–∏–µ
        ]
        
        for pattern in numbered_patterns:
            headers = soup.find_all(['h1', 'h2', 'h3', 'h4'], string=re.compile(pattern))
            for header in headers:
                text = header.get_text().strip()
                match = re.match(pattern, text)
                if match:
                    name = match.group(1).strip()
                    if self._is_valid_place_name(name):
                        place = {
                            'title': name,
                            'detail_url': self._find_place_url(header),
                            'teaser': None,
                            'address_fallback': None,
                            'hours_fallback': None,
                            'number': self._extract_number(text)
                        }
                        places.append(place)
        
        return places
    
    def _extract_from_containers(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –º–µ—Å—Ç –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞—Ö —Å–ø–∏—Å–∫–æ–≤"""
        places = []
        
        # –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —Å –º–µ—Å—Ç–∞–º–∏
        container_selectors = [
            '._listContainer_130k9_1',
            '._container_1k2b4_1',
            '._popularPlaces_1pnw6_1',
            '[class*="list"]',
            '[class*="grid"]',
            '[class*="container"]'
        ]
        
        for selector in container_selectors:
            containers = soup.select(selector)
            for container in containers:
                # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
                headers = container.find_all(['h1', 'h2', 'h3', 'h4', 'h5'])
                for header in headers:
                    text = header.get_text().strip()
                    if self._is_valid_place_name(text) and len(text) > 3:
                        place = {
                            'title': text,
                            'detail_url': self._find_place_url(header),
                            'teaser': None,
                            'address_fallback': None,
                            'hours_fallback': None,
                            'number': len(places) + 1
                        }
                        places.append(place)
        
        return places
    
    def _extract_restaurant_links(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã"""
        places = []
        
        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã
        links = soup.find_all('a', href=re.compile(r'/bangkok/restaurants/'))
        
        for link in links:
            href = link.get('href')
            text = link.get_text().strip()
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Å—ã–ª–∫–∏
            if any(social in href.lower() for social in ['facebook', 'twitter', 'pinterest', 'share', 'getyourguide']):
                continue
            
            if self._is_valid_place_name(text) and len(text) > 3:
                full_url = href if href.startswith('http') else self.base_url + href
                place = {
                    'title': text,
                    'detail_url': full_url,
                    'teaser': None,
                    'address_fallback': None,
                    'hours_fallback': None,
                    'number': len(places) + 1
                }
                places.append(place)
        
        return places
    
    def _extract_from_json_ld(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –º–µ—Å—Ç –≤ JSON-LD —Å—Ç—Ä—É–∫—Ç—É—Ä–∞—Ö"""
        places = []
        
        json_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                if isinstance(data, dict) and 'itemListElement' in data:
                    for item in data['itemListElement']:
                        if 'name' in item:
                            name = item['name']
                            if self._is_valid_place_name(name):
                                place = {
                                    'title': name,
                                    'detail_url': item.get('url', ''),
                                    'teaser': None,
                                    'address_fallback': None,
                                    'hours_fallback': None,
                                    'number': len(places) + 1
                                }
                                places.append(place)
            except (json.JSONDecodeError, KeyError):
                continue
        
        return places
    
    def _is_valid_place_name(self, name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –≤–∞–ª–∏–¥–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º –º–µ—Å—Ç–∞"""
        if not name or len(name) < 3:
            return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
        excluded = [
            'read more', 'photograph:', 'time out', 'bangkok', 'restaurants',
            'cafes', 'best', 'top', 'guide', 'list', 'share', 'facebook',
            'twitter', 'pinterest', 'instagram', 'youtube', 'tiktok',
            'email', 'whatsapp', 'read review', 'photo:', 'jason lang',
            'sereechai puttes', 'nuti pramoch', 'casual eateries',
            'bakeries to find perfect sourdough bread',
            '5 bakeries to find perfect sourdough bread',
            'freeform expressions of neo-indian cuisine',
            'the dk experience', 'review: what to expect from the shake shack x potong collab'
        ]
        
        name_lower = name.lower().strip()
        if any(excluded_word in name_lower for excluded_word in excluded):
            return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–ª–∏ –¥–ª–∏–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        if len(name) < 3 or len(name) > 100:
            return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è, —Å–æ—Å—Ç–æ—è—â–∏–µ —Ç–æ–ª—å–∫–æ –∏–∑ —Ü–∏—Ñ—Ä –∏ —Å–∏–º–≤–æ–ª–æ–≤
        if re.match(r'^[\d\s\.\-_]+$', name):
            return False
        
        return True
    
    def _find_place_url(self, element) -> str:
        """–ü–æ–∏—Å–∫ URL –º–µ—Å—Ç–∞ –≤ —ç–ª–µ–º–µ–Ω—Ç–µ"""
        # –ò—â–µ–º —Å—Å—ã–ª–∫—É –≤ —Å–∞–º–æ–º —ç–ª–µ–º–µ–Ω—Ç–µ
        link = element.find('a')
        if link and link.get('href'):
            href = link.get('href')
            return href if href.startswith('http') else self.base_url + href
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫—É –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–º —ç–ª–µ–º–µ–Ω—Ç–µ
        parent = element.parent
        if parent:
            link = parent.find('a')
            if link and link.get('href'):
                href = link.get('href')
                return href if href.startswith('http') else self.base_url + href
        
        return ""
    
    def _extract_number(self, text: str) -> int:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        match = re.search(r'(\d+)', text)
        return int(match.group(1)) if match else 0
    
    def _remove_duplicates(self, places: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é"""
        seen = set()
        unique_places = []
        
        for place in places:
            name_key = place['title'].lower().strip()
            if name_key not in seen:
                seen.add(name_key)
                unique_places.append(place)
        
        return unique_places

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    adapter = EnhancedTimeOutAdapter()
    
    test_urls = [
        'https://www.timeout.com/bangkok/restaurants/best-restaurants-and-cafes-asoke',
        'https://www.timeout.com/bangkok/restaurants/best-places-to-eat-iconsiam',
        'https://www.timeout.com/bangkok/restaurants/bangkoks-best-new-cafes-of-2025'
    ]
    
    for url in test_urls:
        places = adapter.parse_list_page(url)
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è {url}:")
        for i, place in enumerate(places[:5], 1):
            print(f"  {i}. {place['title']} -> {place['detail_url']}")
        print(f"  ... –∏ –µ—â–µ {len(places) - 5} –º–µ—Å—Ç" if len(places) > 5 else "")
